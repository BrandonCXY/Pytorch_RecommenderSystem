{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateDataset(Dataset):\n",
    "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasMF(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(BiasMF, self).__init__()\n",
    "        self.num_users = params['num_users']\n",
    "        self.num_items = params['num_items']\n",
    "        self.latent_dim = params['latent_dim']\n",
    "        self.mu = params['global_mean'] # mean of all the ratings 不需要学习\n",
    "        self.mu = torch.tensor(self.mu).to(device='cuda')\n",
    "\n",
    "        self.user_embedding = torch.nn.Embedding(self.num_users, self.latent_dim)\n",
    "        self.item_embedding = torch.nn.Embedding(self.num_items, self.latent_dim)\n",
    "        \n",
    "        # 这里的bias是为了消除user的打分bias 需要学习（需要update）\n",
    "        self.user_bias = torch.nn.Embedding(self.num_users, 1)\n",
    "        self.user_bias.weight.data = torch.zeros(self.num_users, 1).float() #updaet\n",
    "        self.item_bias = torch.nn.Embedding(self.num_items, 1)\n",
    "        self.item_bias.weight.data = torch.zeros(self.num_items, 1).float()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_vec = self.user_embedding(user_indices)\n",
    "        item_vec = self.item_embedding(item_indices)\n",
    "        mu = self.mu.view(-1)\n",
    "        \n",
    "        dot = torch.mul(user_vec, item_vec).sum(dim=1)\n",
    "        rating = dot + self.user_bias(user_indices).view(-1) + self.item_bias(item_indices).view(-1) + mu\n",
    "\n",
    "        return rating\n",
    "    # 最好可以直接把bias写在forward中的return里\n",
    "    def get_bias(self, user_indices, item_indices):\n",
    "        return self.user_bias(user_indices).view(-1), self.item_bias(item_indices).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(df, k, learning_rate):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_users = df.userId.unique().shape[0]\n",
    "    num_items = df.movieId.unique().shape[0]\n",
    "    \n",
    "    # user 不需要lookup table\n",
    "    item_unique = set(df.movieId)\n",
    "    item_lookup_table = {item:i for i,item in enumerate(item_unique)}\n",
    "    \n",
    "    for i in range(0,k):\n",
    "        \n",
    "        # k-folds validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state = i)\n",
    "        #loss_sum = 0\n",
    "        for train_index , test_index in kf.split(df.loc[:,['userId','movieId','timestamp']]):\n",
    "            X_train = df.iloc[train_index,[0,1]]\n",
    "            y_train = df.iloc[train_index,[2]]\n",
    "            X_test = df.iloc[test_index,[0,1]]\n",
    "            y_test = df.iloc[test_index,[2]]\n",
    "\n",
    "            params = {'num_users':num_users,'num_items':num_items,'latent_dim':30, 'global_mean':y_train.mean()}\n",
    "\n",
    "            # tensors of training dataset\n",
    "            train_user_tensor = torch.LongTensor(X_train.iloc[:,[0]].values - 1).squeeze()\n",
    "            train_item_tensor = torch.LongTensor([item_lookup_table[X_train.iloc[i,1]] for i in range(len(X_train))])\n",
    "            train_rating_tensor = torch.FloatTensor(y_train.values).squeeze()\n",
    "            \n",
    "            # tensors of test dataset\n",
    "            test_user_tensor = torch.LongTensor(X_test.iloc[:,[0]].values - 1).squeeze()\n",
    "            test_item_tensor = torch.LongTensor([item_lookup_table[X_test.iloc[i,1]] for i in range(len(X_test))])\n",
    "            test_rating_tensor = torch.FloatTensor(y_test.values).squeeze()\n",
    "            \n",
    "            # transform to dataloader\n",
    "            # batch_size need to be big(e.g. batch_size = 10 is not appropriate)\n",
    "            train_dataset = RateDataset(train_user_tensor, train_item_tensor, train_rating_tensor)\n",
    "            train_iter = DataLoader(train_dataset, batch_size = 64, shuffle = True) # train iter has to be shuffled\n",
    "            \n",
    "            test_dataset = RateDataset(test_user_tensor, test_item_tensor, test_rating_tensor)\n",
    "            test_iter = DataLoader(test_dataset, batch_size = 64, shuffle = False)\n",
    "            \n",
    "            # resetting the BiasMF model\n",
    "            model = BiasMF(params).to(device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate, weight_decay = 1e-5, momentum = 0.9)\n",
    "            \n",
    "            # train the model-----------------------------------------\n",
    "            num_epoch = 30\n",
    "            loss_epoch = [0 for j in range(num_epoch)]\n",
    "            for epoch in range(num_epoch):\n",
    "                \n",
    "                for bid, batch in enumerate(train_iter):\n",
    "                    u, i, r = batch[0], batch[1], batch[2]\n",
    "                    u = u.to(device)\n",
    "                    i = i.to(device)\n",
    "                    r = r.to(device)\n",
    "                    r = r. view(-1)\n",
    "                    \n",
    "                    # forward pass\n",
    "                    preds = model(u,i)\n",
    "                    user_bias, item_bias = model.get_bias(u,i)\n",
    "                    # 这里的错误是：\n",
    "                    # 1. u 作为input没有经过embedding就放入正则项，没有意义\n",
    "                    # 2. 还是用optimizer里通过weight decay来自动加正则项比价好\n",
    "                    '''loss = torch.mean(torch.sum(torch.pow(r-preds,2) + \n",
    "                                                l*(torch.pow(u,2)+torch.pow(i,2)+\n",
    "                                                   torch.pow(user_bias,2)+torch.pow(item_bias,2))))'''\n",
    "                    loss = criterion(r, preds)\n",
    "                    # backward and optimize\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                loss_epoch[epoch] = torch.sqrt(loss).item()\n",
    "            \n",
    "                print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1,num_epoch, loss.item()))\n",
    "            #print('train rmse: ', torch.sqrt(criterion(r,preds)))\n",
    "            \n",
    "            # test the model---------------------------------------------\n",
    "\n",
    "            pred_array = np.array([])\n",
    "            r_array = np.array([])\n",
    "            with torch.no_grad():\n",
    "                for bid, batch in enumerate(test_iter):\n",
    "                    u, i, r = batch[0], batch[1], batch[2] \n",
    "                    u = u.to(device)\n",
    "                    i = i.to(device)\n",
    "                    r = r.to(device)\n",
    "                    r = r.view(-1,1)\n",
    "\n",
    "                    preds = model(u,i).view(-1,1)\n",
    "                    \n",
    "                    pred_array = np.append(pred_array,np.array(preds.cpu()))\n",
    "                    r_array = np.append(r_array,np.array(r.cpu()))\n",
    "                    \n",
    "                pred_tensor = torch.FloatTensor(pred_array).to(device)\n",
    "                r_tensor = torch.FloatTensor(r_array).to(device)\n",
    "                \n",
    "                test_rmse = torch.sqrt(criterion(r_tensor, pred_tensor))\n",
    "                print(test_rmse.item())\n",
    "                    \n",
    "    print('finish')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 12.1688\n",
      "Epoch [2/30], Loss: 7.8966\n",
      "Epoch [3/30], Loss: 5.6886\n",
      "Epoch [4/30], Loss: 2.6248\n",
      "Epoch [5/30], Loss: 1.9083\n",
      "Epoch [6/30], Loss: 1.5713\n",
      "Epoch [7/30], Loss: 0.6572\n",
      "Epoch [8/30], Loss: 0.3590\n",
      "Epoch [9/30], Loss: 0.5182\n",
      "Epoch [10/30], Loss: 0.2703\n",
      "Epoch [11/30], Loss: 0.3564\n",
      "Epoch [12/30], Loss: 0.2577\n",
      "Epoch [13/30], Loss: 0.2307\n",
      "Epoch [14/30], Loss: 0.2564\n",
      "Epoch [15/30], Loss: 0.5213\n",
      "Epoch [16/30], Loss: 0.1961\n",
      "Epoch [17/30], Loss: 0.3075\n",
      "Epoch [18/30], Loss: 0.1296\n",
      "Epoch [19/30], Loss: 0.3125\n",
      "Epoch [20/30], Loss: 0.2909\n",
      "Epoch [21/30], Loss: 0.1467\n",
      "Epoch [22/30], Loss: 0.2819\n",
      "Epoch [23/30], Loss: 0.2608\n",
      "Epoch [24/30], Loss: 0.1290\n",
      "Epoch [25/30], Loss: 0.1394\n",
      "Epoch [26/30], Loss: 0.0806\n",
      "Epoch [27/30], Loss: 0.1853\n",
      "Epoch [28/30], Loss: 0.1888\n",
      "Epoch [29/30], Loss: 0.1201\n",
      "Epoch [30/30], Loss: 0.1189\n",
      "1.1208369731903076\n",
      "Epoch [1/30], Loss: 13.0494\n",
      "Epoch [2/30], Loss: 5.9902\n",
      "Epoch [3/30], Loss: 4.7373\n",
      "Epoch [4/30], Loss: 2.0696\n",
      "Epoch [5/30], Loss: 1.0100\n",
      "Epoch [6/30], Loss: 1.0543\n",
      "Epoch [7/30], Loss: 0.6499\n",
      "Epoch [8/30], Loss: 0.6016\n",
      "Epoch [9/30], Loss: 0.3899\n",
      "Epoch [10/30], Loss: 0.4652\n",
      "Epoch [11/30], Loss: 0.5743\n",
      "Epoch [12/30], Loss: 0.5880\n",
      "Epoch [13/30], Loss: 0.3783\n",
      "Epoch [14/30], Loss: 0.4623\n",
      "Epoch [15/30], Loss: 0.2383\n",
      "Epoch [16/30], Loss: 0.2964\n",
      "Epoch [17/30], Loss: 0.2349\n",
      "Epoch [18/30], Loss: 0.2079\n",
      "Epoch [19/30], Loss: 0.2331\n",
      "Epoch [20/30], Loss: 0.0709\n",
      "Epoch [21/30], Loss: 0.1448\n",
      "Epoch [22/30], Loss: 0.1924\n",
      "Epoch [23/30], Loss: 0.1814\n",
      "Epoch [24/30], Loss: 0.1120\n",
      "Epoch [25/30], Loss: 0.1931\n",
      "Epoch [26/30], Loss: 0.1180\n",
      "Epoch [27/30], Loss: 0.1495\n",
      "Epoch [28/30], Loss: 0.1714\n",
      "Epoch [29/30], Loss: 0.1745\n",
      "Epoch [30/30], Loss: 0.2417\n",
      "1.116875171661377\n",
      "Epoch [1/30], Loss: 13.4737\n",
      "Epoch [2/30], Loss: 8.3669\n",
      "Epoch [3/30], Loss: 3.5550\n",
      "Epoch [4/30], Loss: 3.4629\n",
      "Epoch [5/30], Loss: 1.6745\n",
      "Epoch [6/30], Loss: 0.8540\n",
      "Epoch [7/30], Loss: 1.2593\n",
      "Epoch [8/30], Loss: 1.1273\n",
      "Epoch [9/30], Loss: 0.6515\n",
      "Epoch [10/30], Loss: 0.6197\n",
      "Epoch [11/30], Loss: 0.9094\n",
      "Epoch [12/30], Loss: 0.5385\n",
      "Epoch [13/30], Loss: 0.2023\n",
      "Epoch [14/30], Loss: 0.5297\n",
      "Epoch [15/30], Loss: 0.3180\n",
      "Epoch [16/30], Loss: 0.1507\n",
      "Epoch [17/30], Loss: 0.2470\n",
      "Epoch [18/30], Loss: 0.2112\n",
      "Epoch [19/30], Loss: 0.2133\n",
      "Epoch [20/30], Loss: 0.2512\n",
      "Epoch [21/30], Loss: 0.2604\n",
      "Epoch [22/30], Loss: 0.1732\n",
      "Epoch [23/30], Loss: 0.1907\n",
      "Epoch [24/30], Loss: 0.1384\n",
      "Epoch [25/30], Loss: 0.1628\n",
      "Epoch [26/30], Loss: 0.1781\n",
      "Epoch [27/30], Loss: 0.0973\n",
      "Epoch [28/30], Loss: 0.1953\n",
      "Epoch [29/30], Loss: 0.2345\n",
      "Epoch [30/30], Loss: 0.1552\n",
      "1.1195826530456543\n",
      "Epoch [1/30], Loss: 13.7546\n",
      "Epoch [2/30], Loss: 6.1233\n",
      "Epoch [3/30], Loss: 3.2367\n",
      "Epoch [4/30], Loss: 3.2383\n",
      "Epoch [5/30], Loss: 2.0778\n",
      "Epoch [6/30], Loss: 1.6701\n",
      "Epoch [7/30], Loss: 1.0954\n",
      "Epoch [8/30], Loss: 0.3661\n",
      "Epoch [9/30], Loss: 0.5845\n",
      "Epoch [10/30], Loss: 0.5817\n",
      "Epoch [11/30], Loss: 0.7646\n",
      "Epoch [12/30], Loss: 0.3231\n",
      "Epoch [13/30], Loss: 0.3986\n",
      "Epoch [14/30], Loss: 0.3816\n",
      "Epoch [15/30], Loss: 0.2769\n",
      "Epoch [16/30], Loss: 0.3288\n",
      "Epoch [17/30], Loss: 0.3704\n",
      "Epoch [18/30], Loss: 0.3422\n",
      "Epoch [19/30], Loss: 0.2234\n",
      "Epoch [20/30], Loss: 0.3406\n",
      "Epoch [21/30], Loss: 0.1482\n",
      "Epoch [22/30], Loss: 0.1354\n",
      "Epoch [23/30], Loss: 0.2449\n",
      "Epoch [24/30], Loss: 0.2779\n",
      "Epoch [25/30], Loss: 0.2170\n",
      "Epoch [26/30], Loss: 0.1919\n",
      "Epoch [27/30], Loss: 0.1544\n",
      "Epoch [28/30], Loss: 0.1458\n",
      "Epoch [29/30], Loss: 0.1846\n",
      "Epoch [30/30], Loss: 0.1313\n",
      "1.1136012077331543\n",
      "Epoch [1/30], Loss: 19.6914\n",
      "Epoch [2/30], Loss: 5.2214\n",
      "Epoch [3/30], Loss: 4.5501\n",
      "Epoch [4/30], Loss: 3.3914\n",
      "Epoch [5/30], Loss: 2.0651\n",
      "Epoch [6/30], Loss: 1.5744\n",
      "Epoch [7/30], Loss: 0.9466\n",
      "Epoch [8/30], Loss: 0.7594\n",
      "Epoch [9/30], Loss: 0.5495\n",
      "Epoch [10/30], Loss: 0.5680\n",
      "Epoch [11/30], Loss: 0.3876\n",
      "Epoch [12/30], Loss: 0.2241\n",
      "Epoch [13/30], Loss: 0.3276\n",
      "Epoch [14/30], Loss: 0.6298\n",
      "Epoch [15/30], Loss: 0.7447\n",
      "Epoch [16/30], Loss: 0.2550\n",
      "Epoch [17/30], Loss: 0.1377\n",
      "Epoch [18/30], Loss: 0.1496\n",
      "Epoch [19/30], Loss: 0.2470\n",
      "Epoch [20/30], Loss: 0.1552\n",
      "Epoch [21/30], Loss: 0.2063\n",
      "Epoch [22/30], Loss: 0.1976\n",
      "Epoch [23/30], Loss: 0.0971\n",
      "Epoch [24/30], Loss: 0.2915\n",
      "Epoch [25/30], Loss: 0.2753\n",
      "Epoch [26/30], Loss: 0.1389\n",
      "Epoch [27/30], Loss: 0.1150\n",
      "Epoch [28/30], Loss: 0.1901\n",
      "Epoch [29/30], Loss: 0.1896\n",
      "Epoch [30/30], Loss: 0.0953\n",
      "1.1296051740646362\n",
      "Epoch [1/30], Loss: 9.3205\n",
      "Epoch [2/30], Loss: 9.0075\n",
      "Epoch [3/30], Loss: 2.9061\n",
      "Epoch [4/30], Loss: 4.3592\n",
      "Epoch [5/30], Loss: 2.6847\n",
      "Epoch [6/30], Loss: 1.4263\n",
      "Epoch [7/30], Loss: 0.7915\n",
      "Epoch [8/30], Loss: 0.8130\n",
      "Epoch [9/30], Loss: 0.7295\n",
      "Epoch [10/30], Loss: 0.3800\n",
      "Epoch [11/30], Loss: 0.4797\n",
      "Epoch [12/30], Loss: 0.2527\n",
      "Epoch [13/30], Loss: 0.2985\n",
      "Epoch [14/30], Loss: 0.2024\n",
      "Epoch [15/30], Loss: 0.5643\n",
      "Epoch [16/30], Loss: 0.3434\n",
      "Epoch [17/30], Loss: 0.1643\n",
      "Epoch [18/30], Loss: 0.1954\n",
      "Epoch [19/30], Loss: 0.2440\n",
      "Epoch [20/30], Loss: 0.2374\n",
      "Epoch [21/30], Loss: 0.1414\n",
      "Epoch [22/30], Loss: 0.1930\n",
      "Epoch [23/30], Loss: 0.1325\n",
      "Epoch [24/30], Loss: 0.1317\n",
      "Epoch [25/30], Loss: 0.2800\n",
      "Epoch [26/30], Loss: 0.0999\n",
      "Epoch [27/30], Loss: 0.1351\n",
      "Epoch [28/30], Loss: 0.1072\n",
      "Epoch [29/30], Loss: 0.2942\n",
      "Epoch [30/30], Loss: 0.1150\n",
      "1.119297742843628\n",
      "Epoch [1/30], Loss: 15.6550\n",
      "Epoch [2/30], Loss: 9.0767\n",
      "Epoch [3/30], Loss: 3.3507\n",
      "Epoch [4/30], Loss: 2.4130\n",
      "Epoch [5/30], Loss: 0.8277\n",
      "Epoch [6/30], Loss: 0.8429\n",
      "Epoch [7/30], Loss: 0.6560\n",
      "Epoch [8/30], Loss: 0.7857\n",
      "Epoch [9/30], Loss: 0.5577\n",
      "Epoch [10/30], Loss: 0.3299\n",
      "Epoch [11/30], Loss: 0.3567\n",
      "Epoch [12/30], Loss: 0.4916\n",
      "Epoch [13/30], Loss: 0.2523\n",
      "Epoch [14/30], Loss: 0.2539\n",
      "Epoch [15/30], Loss: 0.3209\n",
      "Epoch [16/30], Loss: 0.2538\n",
      "Epoch [17/30], Loss: 0.1803\n",
      "Epoch [18/30], Loss: 0.1956\n",
      "Epoch [19/30], Loss: 0.2830\n",
      "Epoch [20/30], Loss: 0.3924\n",
      "Epoch [21/30], Loss: 0.2071\n",
      "Epoch [22/30], Loss: 0.1655\n",
      "Epoch [23/30], Loss: 0.0772\n",
      "Epoch [24/30], Loss: 0.1331\n",
      "Epoch [25/30], Loss: 0.0957\n",
      "Epoch [26/30], Loss: 0.1945\n",
      "Epoch [27/30], Loss: 0.2173\n",
      "Epoch [28/30], Loss: 0.1089\n",
      "Epoch [29/30], Loss: 0.1272\n",
      "Epoch [30/30], Loss: 0.2476\n",
      "1.116654396057129\n",
      "Epoch [1/30], Loss: 20.5629\n",
      "Epoch [2/30], Loss: 9.5181\n",
      "Epoch [3/30], Loss: 5.0315\n",
      "Epoch [4/30], Loss: 3.1360\n",
      "Epoch [5/30], Loss: 1.3929\n",
      "Epoch [6/30], Loss: 2.2285\n",
      "Epoch [7/30], Loss: 0.9083\n",
      "Epoch [8/30], Loss: 0.4739\n",
      "Epoch [9/30], Loss: 0.3206\n",
      "Epoch [10/30], Loss: 0.6354\n",
      "Epoch [11/30], Loss: 0.6210\n",
      "Epoch [12/30], Loss: 0.3884\n",
      "Epoch [13/30], Loss: 0.3007\n",
      "Epoch [14/30], Loss: 0.2376\n",
      "Epoch [15/30], Loss: 0.5663\n",
      "Epoch [16/30], Loss: 0.4272\n",
      "Epoch [17/30], Loss: 0.4618\n",
      "Epoch [18/30], Loss: 0.1519\n",
      "Epoch [19/30], Loss: 0.3905\n",
      "Epoch [20/30], Loss: 0.2732\n",
      "Epoch [21/30], Loss: 0.1845\n",
      "Epoch [22/30], Loss: 0.0572\n",
      "Epoch [23/30], Loss: 0.1164\n",
      "Epoch [24/30], Loss: 0.1802\n",
      "Epoch [25/30], Loss: 0.1275\n",
      "Epoch [26/30], Loss: 0.1073\n",
      "Epoch [27/30], Loss: 0.1944\n",
      "Epoch [28/30], Loss: 0.1419\n",
      "Epoch [29/30], Loss: 0.0685\n",
      "Epoch [30/30], Loss: 0.1322\n",
      "1.1227525472640991\n",
      "Epoch [1/30], Loss: 10.7049\n",
      "Epoch [2/30], Loss: 7.5857\n",
      "Epoch [3/30], Loss: 4.5400\n",
      "Epoch [4/30], Loss: 1.3638\n",
      "Epoch [5/30], Loss: 1.4292\n",
      "Epoch [6/30], Loss: 1.1293\n",
      "Epoch [7/30], Loss: 0.7373\n",
      "Epoch [8/30], Loss: 0.7342\n",
      "Epoch [9/30], Loss: 0.3044\n",
      "Epoch [10/30], Loss: 0.4010\n",
      "Epoch [11/30], Loss: 0.3670\n",
      "Epoch [12/30], Loss: 0.4551\n",
      "Epoch [13/30], Loss: 0.1789\n",
      "Epoch [14/30], Loss: 0.1406\n",
      "Epoch [15/30], Loss: 0.2658\n",
      "Epoch [16/30], Loss: 0.2505\n",
      "Epoch [17/30], Loss: 0.2675\n",
      "Epoch [18/30], Loss: 0.1658\n",
      "Epoch [19/30], Loss: 0.2705\n",
      "Epoch [20/30], Loss: 0.2128\n",
      "Epoch [21/30], Loss: 0.2909\n",
      "Epoch [22/30], Loss: 0.2967\n",
      "Epoch [23/30], Loss: 0.1428\n",
      "Epoch [24/30], Loss: 0.1918\n",
      "Epoch [25/30], Loss: 0.2447\n",
      "Epoch [26/30], Loss: 0.1018\n",
      "Epoch [27/30], Loss: 0.0973\n",
      "Epoch [28/30], Loss: 0.2967\n",
      "Epoch [29/30], Loss: 0.1482\n",
      "Epoch [30/30], Loss: 0.1845\n",
      "1.106687068939209\n",
      "Epoch [1/30], Loss: 6.6552\n",
      "Epoch [2/30], Loss: 8.0601\n",
      "Epoch [3/30], Loss: 4.9345\n",
      "Epoch [4/30], Loss: 2.5947\n",
      "Epoch [5/30], Loss: 1.4162\n",
      "Epoch [6/30], Loss: 1.6081\n",
      "Epoch [7/30], Loss: 0.5061\n",
      "Epoch [8/30], Loss: 0.7610\n",
      "Epoch [9/30], Loss: 0.8609\n",
      "Epoch [10/30], Loss: 0.2790\n",
      "Epoch [11/30], Loss: 0.6173\n",
      "Epoch [12/30], Loss: 0.6393\n",
      "Epoch [13/30], Loss: 0.3456\n",
      "Epoch [14/30], Loss: 0.3228\n",
      "Epoch [15/30], Loss: 0.0987\n",
      "Epoch [16/30], Loss: 0.3072\n",
      "Epoch [17/30], Loss: 0.3109\n",
      "Epoch [18/30], Loss: 0.3163\n",
      "Epoch [19/30], Loss: 0.2791\n",
      "Epoch [20/30], Loss: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Loss: 0.2844\n",
      "Epoch [22/30], Loss: 0.1487\n",
      "Epoch [23/30], Loss: 0.1600\n",
      "Epoch [24/30], Loss: 0.2020\n",
      "Epoch [25/30], Loss: 0.1035\n",
      "Epoch [26/30], Loss: 0.1153\n",
      "Epoch [27/30], Loss: 0.2608\n",
      "Epoch [28/30], Loss: 0.1663\n",
      "Epoch [29/30], Loss: 0.0722\n",
      "Epoch [30/30], Loss: 0.0990\n",
      "1.125557780265808\n",
      "finish\n",
      "cost time: 614.9383878707886\n"
     ]
    }
   ],
   "source": [
    "# lr = 1e-5, lambda= 1e-8, k folds = 3, random states = i, batch_size = 64, epoch = 70 with weight decay = 1e-5\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "df = pd.read_csv('/home/dm/Downloads/movielens100k/ratings.csv')\n",
    "train_and_predict(df, 2, 0.002)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print('cost time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“torch0”",
   "language": "python",
   "name": "torch0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
